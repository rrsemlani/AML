{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification \n",
    "\n",
    "In this assignment, you will:\n",
    "\n",
    "1. Propose a custom text classification task and create a corpus of documents that are labeled for this task.\n",
    "\n",
    "2. Propose discriminative features to be included in the feature vector representation for a document and evaluate their utility by training and testing Logistic Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Your Name Here: Mukesh Dasari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus acquisition and formatting\n",
    "\n",
    "1. Find at least 300 documents for some topic that interests you, along with a single label for each document.\n",
    "\n",
    "\t- The more data in your collection, the better your classification models will tend to perform on it.\n",
    "    \n",
    "\n",
    "2. Partition your data into three files train.txt, dev.txt and test.txt, with training containing 80% of the documents, development 10% and test 10%.\n",
    "\n",
    "\n",
    "3. All of the data must be in a common format:\n",
    "\n",
    "    - One example per line, using the format \"\\<label\\> \\<text\\>\", i.e. the line starts with the label, followed by a white space, followed by the text of the document.\n",
    "    - Replace all newlines in the text with \\<NEWLINE\\> and tab characters with \\<TAB\\>.\n",
    "    - See corpus under *data/sentiment/* for examples.\n",
    "    \n",
    "    \n",
    "4. Your choice of documents and labels is completely up to you. Possible sources of data:\n",
    "\n",
    "    - **Project Gutenberg**: Metadata is available at this <a href=\"https://github.com/hugovk/gutenberg-metadata\">Github repo</a> along with URLs for the texts. Labels here can be author, subject, genre, etc.\n",
    "\t- **News articles**: Crawl news articles from different domains (e.g,. CNN, FoxNews); the label for each article is the domain.\n",
    "\t- **Movie summaries**: Labels here can be any categorical metadata aspect (genre, release date); note real-valued metadata (like box office, runtime) can be discretized by selecting some reasonable thresholds.\n",
    "\t- **Tweets**: Download your own tweets. Labels here can be any categorical metadata included in the tweet, or labels you add by hand (e.g., sarcasm).\n",
    "    \n",
    "    \n",
    "5. Additional requirements:\n",
    "\n",
    "    - No sentiment classification.\n",
    "    - **Undergraduate students**: It is acceptable to use an existing dataset, e.g. from kaggle.com or other repositories. However, it is preferable that you create your own dataset.\n",
    "    - **Graduate students**: It is important that you create your own dataset. This can also serve as the basis for your project, if you choose to work on text classification for it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task and Dataset description\n",
    "\n",
    "Describe your data. What is the source of the documents, and what do the labels mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "In this dataset, the tweets and retweets from different users is gathered which accounts total 1028 in training set, 128 in test and dev set respectively. The source of the data is mainly twitter and some other websites as finding hate speech was challenging on current twitter page. The data is mainly labeled in two classes, hate and normal. Hate label indicates that the tweet or retweet is a hate speech and normal tweet indicates that it is the normal tweet. \n",
    "\n",
    "The main fun was to play around the data which included defining the features, removing new lines which I did while creating the data set itself. \n",
    "\n",
    "**disclaimer - This dataset has many hate and abusive words. I do not promote hate speech on any social media but idea behind gathering this data was to see how to identify if any tweet or retweet is promoting hatred on twitter.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reading and statistics\n",
    "\n",
    "Change the path to match the directory containing your data and execute the *read_examples()* function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: {'hate': 500, 'normal': 528}\n",
      "Development examples: {'hate': 63, 'normal': 67}\n",
      "Test examples: {'hate': 62, 'normal': 66}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    with open(filename, mode = 'r') as file:\n",
    "        urls = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        mentions = '@[\\w\\-]+'\n",
    "        hashtags = '#[\\w\\-]+'\n",
    "        \n",
    "        \n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split('\\t', maxsplit = 1)\n",
    "            text1 = re.sub(urls, 'URL', text)\n",
    "            text1 = re.sub(mentions, 'MENTION', text1)\n",
    "            text1 = re.sub(hashtags, 'HASHTAG', text1)\n",
    "            \n",
    "            X.append(text1)\n",
    "            Y.append(label)\n",
    "            \n",
    "    return X, Y\n",
    "\n",
    "def label_counts(Y):\n",
    "    labels = {}\n",
    "    for l in Y:\n",
    "        if l in labels:\n",
    "            labels[l] += 1\n",
    "        else:\n",
    "            labels[l] = 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "datapath = \"../data/twitter\"\n",
    "train_file = os.path.join(datapath, 'train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "print(\"Training examples:\", label_counts(trainY))\n",
    "\n",
    "\n",
    "dev_file = os.path.join(datapath, 'dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "print(\"Development examples:\", label_counts(devY))\n",
    "\n",
    "\n",
    "test_file = os.path.join(datapath, 'test.txt')\n",
    "testX, testY = read_examples(test_file)\n",
    "print(\"Test examples:\", label_counts(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From documents to feature vectors\n",
    "This section illustratess the prototypical components of machine learning pipeline for an NLP task, in this case document classification:\n",
    "\n",
    "1. Read document examples (train, devel, test) from files with a predefined format:\n",
    "    - assume one document per line, using the format \"\\<label\\> \\<text\\>\".\n",
    "\n",
    "2. Tokenize each document:\n",
    "    - using a spaCy tokenizer.\n",
    "\n",
    "3. Feature extractors:\n",
    "    - so far, just words.\n",
    "\n",
    "4. Process each document into a feature vector:\n",
    "    - map document to a dictionary of feature names.\n",
    "    - map feature names to unique feature IDs.\n",
    "    - each document is a feature vector, where each feature ID is mapped to a feature value (e.g. word occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy import sparse\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spaCy tokenizer.\n",
    "spacy_nlp = English()\n",
    "def spacy_tokenizer(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split('\\t', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(tokens):\n",
    "    feats = {}\n",
    "    for word in tokens:\n",
    "        feat = 'WORD_%s' % word\n",
    "        if feat in feats:\n",
    "            feats[feat] +=1\n",
    "        else:\n",
    "            feats[feat] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feats, new_feats):\n",
    "    for feat in new_feats:\n",
    "        if feat in feats:\n",
    "            feats[feat] += new_feats[feat]\n",
    "        else:\n",
    "            feats[feat] = new_feats[feat]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tokenizes the document, runs all the feature extractors on it and assembles the extracted features into a dictionary mapping feature names to feature values. It is important that feature names do not conflict with each other, i.e. different features should have different names. Each document will have its own dictionary of features and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2features(trainX, feature_functions, tokenizer):\n",
    "    examples = []\n",
    "    count = 0\n",
    "    \n",
    "    for doc in trainX:\n",
    "        feats = {}\n",
    "\n",
    "        tokens = tokenizer(doc)\n",
    "        \n",
    "        for func in feature_functions:\n",
    "            add_features(feats, func(tokens))\n",
    "\n",
    "        examples.append(feats)\n",
    "        count +=1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print('Processed %d examples into features' % len(examples))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts feature names to unique numerical IDs.\n",
    "\n",
    "def create_vocab(examples):\n",
    "    feature_vocab = {}\n",
    "    idx = 0\n",
    "    for example in examples:\n",
    "        for feat in example:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a set of examples from a dictionary of feature names to values representation\n",
    "# to a sparse representation of feature ids to values. This is important because almost all feature values will\n",
    "# be 0 for most documents and it would be wasteful to save all in memory.\n",
    "\n",
    "def features_to_ids(examples, feature_vocab):\n",
    "    new_examples = sparse.lil_matrix((len(examples), len(feature_vocab)))\n",
    "    for idx, example in enumerate(examples):\n",
    "        for feat in example:\n",
    "            if feat in feature_vocab:\n",
    "                new_examples[idx, feature_vocab[feat]] = example[feat]\n",
    "                \n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pipeline for the Logistic Regression classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train LR model.\n",
    "    lr_model = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'lbfgs', max_iter = 1000)\n",
    "    lr_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test LR model.\n",
    "    print('Accuracy: %.3f' % lr_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "Create two new feature functions, and include them in the *features* list.\n",
    "\n",
    "- A passing grade will be given to generic features that apply across arbitrary text classification problems (e.g., a feature for bigrams);\n",
    "\n",
    "- A better grade will be given for features that reveal your own understanding of your data. What features do you think will help for your particular problem? Would features based on higher level NLP processing tasks (anmed entity recognition, syntactic parsing, coreference resolution) be useful? Your grade is not tied to whether accuracy goes up or down, so be creative!\n",
    "\n",
    "- You are free to read in any other external resources you like (dictionaries, document metadata, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_len(tokens):\n",
    "    feat = {'DOC_LEN' : len(tokens)}\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_urls(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if 'URL' in token:\n",
    "            count += 1\n",
    "    return {'URLS':count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_mentions(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if 'MENTION' in token:\n",
    "            count += 1\n",
    "            \n",
    "    return {'MENTIONS':count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_hashtags(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if 'HASHTAG' in token:\n",
    "            count += 1\n",
    "    \n",
    "    return {'HASHTAGS':count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 3791\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.823\n"
     ]
    }
   ],
   "source": [
    "features = [word_features]\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 3792\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.823\n"
     ]
    }
   ],
   "source": [
    "features = [word_features, feature_urls]\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 3793\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.823\n"
     ]
    }
   ],
   "source": [
    "features = [word_features, feature_urls, feature_mentions]\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 3794\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.823\n"
     ]
    }
   ],
   "source": [
    "features = [word_features, feature_urls, feature_mentions, feature_hashtags]\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 3795\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "features = [word_features, feature_urls, feature_mentions, feature_hashtags, feature_len]\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of your features by training and testing with and without each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 5297\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.846\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datapath = '../data/twitter'\n",
    "\n",
    "train_file = os.path.join(datapath, 'train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "\n",
    "dev_file = os.path.join(datapath, 'dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "\n",
    "# Specify features to use. Do this multiple times, with and without the new features\n",
    "features1 = [word_features]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features1, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "\n",
    "*Mandatory for graduate students, optional for undergraduate students.*\n",
    "\n",
    "Take your best classifier and plot a <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\">Receiver Operating Characteristic (ROC)</a> curve by varying a threshold on the probabilistic output. You can use the <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\">sklearn implementation</a>, or implement your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 100 examples into features\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_and_test1(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "     # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Train LR model.\n",
    "    lr_model = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'lbfgs', max_iter = 1000)\n",
    "    lr_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    lr_probs = lr_model.predict_proba(devX_ids)[:, 1]\n",
    "    \n",
    "    devY_num = []\n",
    "    for x in devY:\n",
    "        if x == 'hate':\n",
    "            devY_num.append(0)\n",
    "        else:\n",
    "            devY_num.append(1)\n",
    "            \n",
    "    fpr1, tpr1, thresholds1 = roc_curve(devY_num, lr_probs)\n",
    "    \n",
    "    \n",
    "    # roc curve for classes\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.plot(fpr1, tpr1, label='Logistic Regression', color='red')\n",
    "\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# features = [word_features, feature_len, feature_stopwords]\n",
    "features = [word_features]\n",
    "train_and_test1(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "\n",
    "Anything extra goes here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_retweet(tokens):\n",
    "    retweet = 0\n",
    "    for token in tokens:\n",
    "        if token.lower() == 'rt':\n",
    "            retweet += 1\n",
    "            \n",
    "    return {'RETWEET_COUNT' : retweet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_stopwords(tokens):\n",
    "    stopwords = spacy_nlp.Defaults.stop_words\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            if token.lower() in stopwords:\n",
    "                count += 1\n",
    "    return {'DEICTIC_COUNT':count}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 5302\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.823\n"
     ]
    }
   ],
   "source": [
    "features = [word_features, feature_urls, feature_mentions, feature_hashtags, feature_len, feature_stopwords]\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 5303\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.823\n"
     ]
    }
   ],
   "source": [
    "features = [word_features, feature_urls, feature_mentions, feature_hashtags, feature_len, feature_stopwords, feature_retweet]\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Vocabulary size: 5300\n",
      "Processed 100 examples into features\n",
      "Accuracy: 0.846\n"
     ]
    }
   ],
   "source": [
    "features = [word_features, feature_mentions, feature_hashtags, feature_retweet]\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis ##\n",
    "Include an analysis of the results that you obtained in the experiments above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please refer to the Analysis_Report.pdf   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
